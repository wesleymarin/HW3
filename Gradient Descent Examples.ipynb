{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/site-packages/matplotlib/font_manager.py:273: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\n",
      "  warnings.warn('Matplotlib is building the font cache using fc-list. This may take a moment.')\n",
      "/usr/local/lib/python3.5/site-packages/matplotlib/font_manager.py:273: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\n",
      "  warnings.warn('Matplotlib is building the font cache using fc-list. This may take a moment.')\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets.samples_generator import make_blobs\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid_activation(x):\n",
    "\t# compute and return the sigmoid activation value for a\n",
    "\t# given input value\n",
    "\treturn 1.0 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "alpha = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] starting training...\n"
     ]
    }
   ],
   "source": [
    "# generate a 2-class classification problem with 250 data points,\n",
    "# where each data point is a 2D feature vector\n",
    "(X, y) = make_blobs(n_samples=250, n_features=2, centers=2,\n",
    "\tcluster_std=1.05, random_state=20)\n",
    " \n",
    "# insert a column of 1's as the first entry in the feature\n",
    "# vector -- this is a little trick that allows us to treat\n",
    "# the bias as a trainable parameter *within* the weight matrix\n",
    "# rather than an entirely separate variable\n",
    "X = np.c_[np.ones((X.shape[0])), X]\n",
    " \n",
    "# initialize our weight matrix such it has the same number of\n",
    "# columns as our input features\n",
    "print(\"[INFO] starting training...\")\n",
    "W = np.random.uniform(size=(X.shape[1],))\n",
    " \n",
    "# initialize a list to store the loss value for each epoch\n",
    "lossHistory = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.99979695,  0.9845023 ,  0.98110111,  0.99972054,  0.99994213,\n",
       "        0.86875818,  0.91989282,  0.99956247,  0.99985612,  0.97151699,\n",
       "        0.99986315,  0.99983848,  0.99973673,  0.99982835,  0.9364751 ,\n",
       "        0.99982738,  0.9993743 ,  0.99947887,  0.97108945,  0.99919101,\n",
       "        0.9290198 ,  0.9997429 ,  0.87156958,  0.5594347 ,  0.99985915,\n",
       "        0.99988027,  0.82560664,  0.99979624,  0.99951928,  0.92470357,\n",
       "        0.98231124,  0.99962902,  0.98130837,  0.6499141 ,  0.99996301,\n",
       "        0.99985952,  0.99905537,  0.99967644,  0.99947453,  0.99975238,\n",
       "        0.73176197,  0.999945  ,  0.96930943,  0.99971292,  0.99946522,\n",
       "        0.99906338,  0.99991169,  0.99912338,  0.99972092,  0.99971295,\n",
       "        0.97308453,  0.99940065,  0.99885671,  0.92696712,  0.98358327,\n",
       "        0.99930469,  0.99982463,  0.99970672,  0.97927782,  0.99992518,\n",
       "        0.99993158,  0.99974239,  0.99967719,  0.99961859,  0.98040336,\n",
       "        0.91432176,  0.86007127,  0.86338589,  0.99993541,  0.91760576,\n",
       "        0.95166098,  0.98420383,  0.95652888,  0.958994  ,  0.94794025,\n",
       "        0.97548981,  0.99120178,  0.93239228,  0.87988782,  0.95274224,\n",
       "        0.99928442,  0.98563431,  0.99982806,  0.99890399,  0.94997632,\n",
       "        0.99957362,  0.93052904,  0.99941208,  0.97925498,  0.9999545 ,\n",
       "        0.77249522,  0.99957983,  0.90485027,  0.99983796,  0.95167122,\n",
       "        0.99953622,  0.99979929,  0.93002828,  0.99963791,  0.82920795,\n",
       "        0.9579181 ,  0.87643335,  0.95102043,  0.99992304,  0.9997353 ,\n",
       "        0.99971389,  0.96561805,  0.93642842,  0.97503728,  0.95449034,\n",
       "        0.99992565,  0.99992664,  0.98230872,  0.9995737 ,  0.99956721,\n",
       "        0.97154471,  0.99993099,  0.95173248,  0.93777478,  0.99984528,\n",
       "        0.99993139,  0.97909725,  0.95936189,  0.98194014,  0.92431368,\n",
       "        0.90531128,  0.77191637,  0.97428433,  0.99850675,  0.96415444,\n",
       "        0.99980478,  0.97381719,  0.9997765 ,  0.99990022,  0.99978321,\n",
       "        0.99987102,  0.99981803,  0.81116583,  0.83742632,  0.71141395,\n",
       "        0.98645125,  0.88604979,  0.8764028 ,  0.95449672,  0.99992569,\n",
       "        0.99842864,  0.96634685,  0.99299491,  0.72243907,  0.87141587,\n",
       "        0.9993778 ,  0.9998458 ,  0.96525958,  0.95729227,  0.99047706,\n",
       "        0.99948941,  0.99858214,  0.99977846,  0.95564467,  0.92687968,\n",
       "        0.99975929,  0.99963426,  0.87035995,  0.99997295,  0.99958404,\n",
       "        0.96695259,  0.96651936,  0.99987555,  0.92753726,  0.97953572,\n",
       "        0.99986145,  0.84260817,  0.94558145,  0.98204636,  0.99945053,\n",
       "        0.96783204,  0.96096341,  0.99929594,  0.99908296,  0.9997439 ,\n",
       "        0.99993427,  0.99967373,  0.95409422,  0.99988734,  0.94720554,\n",
       "        0.99988083,  0.99946637,  0.86303834,  0.9996745 ,  0.95039374,\n",
       "        0.99984745,  0.99963934,  0.85665373,  0.99948157,  0.98187445,\n",
       "        0.94947062,  0.99911807,  0.99976281,  0.99950367,  0.99991695,\n",
       "        0.99961627,  0.99995874,  0.9037789 ,  0.99978126,  0.91612395,\n",
       "        0.96806106,  0.99894527,  0.91606164,  0.99982778,  0.99874742,\n",
       "        0.98789856,  0.93384508,  0.96131008,  0.76721613,  0.9998659 ,\n",
       "        0.93908873,  0.96807594,  0.97114696,  0.79051886,  0.94689775,\n",
       "        0.99993759,  0.99924226,  0.99975506,  0.90620664,  0.99993455,\n",
       "        0.99997096,  0.99956645,  0.93747671,  0.99046329,  0.99972483,\n",
       "        0.99982384,  0.99964768,  0.99980497,  0.99993458,  0.9558343 ,\n",
       "        0.98492974,  0.98914644,  0.99993686,  0.85672037,  0.94716724,\n",
       "        0.74757086,  0.94097729,  0.99989445,  0.99979586,  0.99935463,\n",
       "        0.99971829,  0.92118673,  0.85642337,  0.90618917,  0.91203963])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sigmoid_activation(X.dot(W))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
